// memory usage for predict function
Filename: .\artificialneuralnetwork.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    28    191.9 MiB    191.9 MiB           1   @profile
    29                                         def predict(Theta1, Theta2, X):
    30                                          """ Predict labels in a trained three layer classification network.
    31                                          Input:
    32                                            Theta1       trained weights applied to 1st layer (hidden_layer_size x input_layer_size+1)
    33                                            Theta2       trained weights applied to 2nd layer (num_labels x hidden_layer_size+1)
    34                                            X            matrix of training data      (m x input_layer_size)
    35                                          Output:
    36                                            prediction   label prediction
    37                                          """
    38
    39    191.9 MiB      0.0 MiB           1    m = np.shape(X)[0]                    # number of training values
    40    191.9 MiB      0.0 MiB           1    num_labels = np.shape(Theta2)[0]
    41
    42    213.1 MiB     21.1 MiB           1    a1 = np.hstack((np.ones((m,1)), X))   # add bias (input layer)
    43    213.4 MiB      0.3 MiB           1    a2 = g(a1 @ Theta1.T)                 # apply sigmoid: input layer --> hidden layer
    44    213.4 MiB      0.0 MiB           1    a2 = np.hstack((np.ones((m,1)), a2))  # add bias (hidden layer)
    45    213.5 MiB      0.1 MiB           1    a3 = g(a2 @ Theta2.T)                 # apply sigmoid: hidden layer --> output layer
    46
    47    213.5 MiB      0.0 MiB           1    prediction = np.argmax(a3,1).reshape((m,1))
    48    213.5 MiB      0.0 MiB           1    return prediction



//memory usage for cost_function 
Filename: artificialneuralnetwork.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    58    194.1 MiB    194.1 MiB           1   @profile
    59                                         def cost_function(theta, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda):
    60                                          """ Neural net cost function for a three layer classification network.
    61                                          Input:
    62                                            theta               flattened vector of neural net model parameters
    63                                            input_layer_size    size of input layer
    64                                            hidden_layer_size   size of hidden layer
    65                                            num_labels          number of labels
    66                                            X                   matrix of training data
    67                                            y                   vector of training labels
    68                                            lmbda               regularization term
    69                                          Output:
    70                                            J                   cost function
    71                                          """
    72
    73                                          # unflatten theta
    74    194.1 MiB      0.0 MiB           1    Theta1, Theta2 = reshape(theta, input_layer_size, hidden_layer_size, num_labels)
    75
    76                                          # number of training values
    77    194.1 MiB      0.0 MiB           1    m = len(y)
    78
    79                                          # Feedforward: calculate the cost function J:
    80
    81    215.2 MiB     21.1 MiB           1    a1 = np.hstack((np.ones((m,1)), X))
    82    215.2 MiB      0.0 MiB           1    a2 = g(a1 @ Theta1.T)
    83    215.2 MiB      0.0 MiB           1    a2 = np.hstack((np.ones((m,1)), a2))
    84    215.2 MiB      0.0 MiB           1    a3 = g(a2 @ Theta2.T)
    85
    86    215.2 MiB      0.0 MiB           1    y_mtx = 1.*(y==0)
    87    215.2 MiB      0.0 MiB           3    for k in range(1,num_labels):
    88    215.2 MiB      0.0 MiB           2            y_mtx = np.hstack((y_mtx, 1.*(y==k)))
    89
    90                                          # cost function
    91    215.3 MiB      0.1 MiB           1    J = np.sum( -y_mtx * np.log(a3) - (1.0-y_mtx) * np.log(1.0-a3) ) / m
    92
    93                                          # add regularization
    94    215.3 MiB      0.0 MiB           1    J += lmbda/(2.*m) * (np.sum(Theta1[:,1:]**2)  + np.sum(Theta2[:,1:]**2))
    95
    96    215.3 MiB      0.0 MiB           1    return J




//memory usage for gradient function
Filename: artificialneuralnetwork.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
    98    191.7 MiB    191.7 MiB           1   @profile
    99                                         def gradient(theta, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda):
   100                                          """ Neural net cost function gradient for a three layer classification network.
   101                                          Input:
   102                                            theta               flattened vector of neural net model parameters
   103                                            input_layer_size    size of input layer
   104                                            hidden_layer_size   size of hidden layer
   105                                            num_labels          number of labels
   106                                            X                   matrix of training data
   107                                            y                   vector of training labels
   108                                            lmbda               regularization term
   109                                          Output:
   110                                            grad                flattened vector of derivatives of the neural network
   111                                          """
   112
   113                                          # unflatten theta
   114    191.7 MiB      0.0 MiB           1    Theta1, Theta2 = reshape(theta, input_layer_size, hidden_layer_size, num_labels)
   115
   116                                          # number of training values
   117    191.7 MiB      0.0 MiB           1    m = len(y)
   118
   119                                          # Backpropagation: calculate the gradients Theta1_grad and Theta2_grad:
   120
   121    191.7 MiB      0.0 MiB           1    Delta1 = np.zeros((hidden_layer_size,input_layer_size+1))
   122    191.7 MiB      0.0 MiB           1    Delta2 = np.zeros((num_labels,hidden_layer_size+1))
   123
   124    191.7 MiB      0.0 MiB        2701    for t in range(m):
   125
   126                                                  # forward
   127    191.7 MiB      0.0 MiB        2700            a1 = X[t,:].reshape((input_layer_size,1))
   128    191.7 MiB      0.0 MiB        2700            a1 = np.vstack((1, a1))   #  +bias
   129    191.7 MiB      0.0 MiB        2700            z2 = Theta1 @ a1
   130    191.7 MiB      0.0 MiB        2700            a2 = g(z2)
   131    191.7 MiB      0.0 MiB        2700            a2 = np.vstack((1, a2))   #  +bias
   132    191.7 MiB      0.0 MiB        2700            a3 = g(Theta2 @ a2)
   133
   134                                                  # compute error for layer 3
   135    191.7 MiB      0.0 MiB        2700            y_k = np.zeros((num_labels,1))
   136    191.7 MiB      0.0 MiB        2700            y_k[y[t,0].astype(int)] = 1
   137    191.7 MiB      0.0 MiB        2700            delta3 = a3 - y_k
   138    191.7 MiB      0.0 MiB        2700            Delta2 += (delta3 @ a2.T)
   139
   140                                                  # compute error for layer 2
   141    191.7 MiB      0.0 MiB        2700            delta2 = (Theta2[:,1:].T @ delta3) * grad_g(z2)
   142    191.7 MiB      0.0 MiB        2700            Delta1 += (delta2 @ a1.T)
   143
   144    191.7 MiB      0.0 MiB           1    Theta1_grad = Delta1 / m
   145    191.7 MiB      0.0 MiB           1    Theta2_grad = Delta2 / m
   146
   147                                          # add regularization
   148    191.7 MiB      0.0 MiB           1    Theta1_grad[:,1:] += (lmbda/m) * Theta1[:,1:]
   149    191.7 MiB      0.0 MiB           1    Theta2_grad[:,1:] += (lmbda/m) * Theta2[:,1:]
   150
   151                                          # flatten gradients
   152    191.7 MiB      0.0 MiB           1    grad = np.concatenate((Theta1_grad.flatten(), Theta2_grad.flatten()))
   153
   154    191.7 MiB      0.0 MiB           1    return grad